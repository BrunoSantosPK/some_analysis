{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições\n",
    "\n",
    "Este espaço é dedicado para a importação de bibliotecas utilizadas em todas as etapas: listagem de títulos, download da fonte e processamento de informações (*scrap*). Além disso, duas variáveis importantes são inicializadas: **BASE_PATH** e **publishers**.\n",
    "\n",
    "A variável **BASE_PATH** tem como principal função gerenciar os caminhos para salvar arquivos de dados e fontes. Com ela, independente do sistema operacional, é possível fazer com que os arquivos sejam posicionados nos diretórios corretos.\n",
    "\n",
    "Por outro lado **publishers** é a variável que controla as editoras de quadrinhos que terão seu portfólio listado, já que a primeira parte (listagem de títulos) ocorre por meio da pesquisa avançada, utilizando um *query param* para filtrar resultados para uma editora. Isto resulta em listagens menores do que uma listagem ampla, reduzindo casos de truncagem por meio da rota de busca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from datetime import date, datetime\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "BASE_PATH = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers = [\n",
    "    \"pipoca+e+nanquim\",\n",
    "    \"newpop\",\n",
    "    \"jbc\",\n",
    "    \"editora+jbc\",\n",
    "    \"panini\",\n",
    "    \"mythos\",\n",
    "    \"conrad\"\n",
    "]\n",
    "\n",
    "years = list(range(2014, 2025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Listagem de Títulos\n",
    "\n",
    "Como muitas lojas possuem restrições de *requests* feitas automaticamente, um RPA é desenvolvido para acessar a rota de pesquisa e listar todos os títulos encontrados, junto com os seus links para páginas de detalhes. Este processo inicial tem como objetivo obter uma diretiva para saber quais páginas de quais títulos serão acessadas para captura de dados.\n",
    "\n",
    "O resultado desta etapa é um conjunto de dados com os títulos disponíveis e o link para acessar a página de detalhes. Este conjunto é criado em um tempo relativamente curto e precisa de alguns *sleep* para permitir que o JS execute e a página seja completamente criada (a loja não provê páginas estáticas). O tempo de execução é a parte fundamental para esta etapa, pois a próxima é extremamante lenta e possuir uma listagem prévia de quais títulos serão acessados é de grande importância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_visit = True\n",
    "title_dataset = []\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "for publisher in publishers:\n",
    "    for year in years:\n",
    "        url = f\"https://www.amazon.com.br/s?i=stripbooks&rh=n%3A7842710011%2Cp_30%3A{publisher}&s=date-desc-rank&Adv-Srch-Books-Submit.x=38&p_46=During&p_47={year}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Contorna o primeiro acesso bloqueado pelo servidor\n",
    "        if first_visit:\n",
    "            first_visit = False\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "\n",
    "        while True:\n",
    "            # Recupera lista de resultados\n",
    "            elements = driver.find_element(By.CLASS_NAME, \"s-result-list\")\n",
    "            books = elements.find_elements(By.XPATH, \".//div[@data-component-type='s-search-result']\")\n",
    "\n",
    "            # Se não existe resultado na busca, encerra o loop\n",
    "            if len(books) == 0:\n",
    "                break\n",
    "\n",
    "            # Percorre os livros encontrados e salva dados\n",
    "            for book in books:\n",
    "                row = {\"publisher\": \" \".join(publisher.split(\"+\")), \"extract_at\": date.today().isoformat()}\n",
    "\n",
    "                book_link = book.find_element(By.XPATH, \".//a[@class='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal']\")\n",
    "                row[\"page_link\"] = book_link.get_attribute(\"href\")\n",
    "\n",
    "                book_name = book.find_element(By.XPATH, \".//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "                row[\"name\"] = book_name.get_attribute(\"innerHTML\")\n",
    "\n",
    "                title_dataset.append(row)\n",
    "\n",
    "            # Faz a troca de página, utilizando o botão de \"próxima\"\n",
    "            old_url = driver.current_url\n",
    "            try:\n",
    "                driver.find_element(By.CLASS_NAME, \"s-pagination-next\").click()\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            if driver.current_url == old_url:\n",
    "                break\n",
    "\n",
    "title_dataset = pd.DataFrame(title_dataset)\n",
    "driver.close()\n",
    "\n",
    "# Salva os dados primários para utilização posterior\n",
    "title_dataset.to_csv(f\"{BASE_PATH}/data/hqs/titles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: Download de Fonte\n",
    "\n",
    "Como estamos lidando com *webscraping*, muitas lojas criam barreiras para acesso de scripts com requisições de acesso em alto volume. Alguns casos são contornados por meio da edição do *header* dentro da *request* junto de um delay entre cada requisição. Porém, ainda assim existem formas de bloquear tais acessos, o que acontece neste caso. Desta forma, outro RPA foi criado, que automatiza o processo de abertura dos links em um navegador e faz o download do HTML fonte da página.\n",
    "\n",
    "O processo de abrir o link, recuperar o código fonte e salvar em um arquivo .html localmente é lento, principalmente se comparado com o método tradicional utilizando o *requests*. Todavia, como não estamos lidando com volumes altos de dados (são cerca de 3000 quadrinhos encontrados na etapa anterior) e como não existe necessidade de redownload, este processo ainda é considerado tratável, ou pelo menos \"suportável\".\n",
    "\n",
    "Foi brevemente comentado que a listagem prévia seria importante nesta parte e o motivo é para a manutenção do *log*. Ao se trabalhar com páginas da internet e processos lento, manter um *log* de execução para ser possível identificar o que foi feito e o que falta fazer é de necessidade absoluta. Em qualquer falha no processo, seja por não carregamento, falha de memória ou até mesmo a interrupção manual, as páginas baixadas até o momento não serão revisitadas em uma próxima execução. Isto também permite que a Parte 1 seja atualizada sem que a Parte 2 precise reexecutar todo o download. Em termos mais específicos, o download é feito no modo *append*, sempre baixando coisas novas e nunca avaliando estruturas já baixadas. Novamente, este modo de operação é viável porque os dados que queremos são físicos (preço de capa, editora, formato e número de páginas) que não se alteram com o tempo.\n",
    "\n",
    "O *log* de download é armazenado em um arquivo .txt que contêm apenas o título do produto processado e o tempo gasto no processo de abrir o navegador, acessar o link, baixar o código fonte e salvar em arquivo .html. Mesmo que o processo de atualização do arquivo de *log* seja uma atividade extra que pode tornar o processo mais lento, ele ainda é melhor do que assumir o risco de falha e ser necessário reiniciar a etapa lenta do início.\n",
    "\n",
    "Por fim, a necessidade de fazer o download e não executar o processamento (*scraping*) ao mesmo tempo é concentrar o processo lento em uma etapa isolada, configurando uma boa prática. De fato, o processamento pode necessitar de várias flags e, de acordo com necessidades futuras, pode ser alterado. Se estivesse atrelado ao download, o mesmo teria que ou ser replicado fora do processo ou executado junto com um novo download, aplicando novamente um processo lento para uma mudança que deveria de ser rápida, uma vez que é uma alteração de processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acessando 1983/1983\r"
     ]
    }
   ],
   "source": [
    "title_dataset = pd.read_csv(f\"{BASE_PATH}/data/hqs/titles.csv\")\n",
    "driver = webdriver.Chrome()\n",
    "first_visit = True\n",
    "\n",
    "# Verifica se existe um log de andamento de download\n",
    "log_path = f\"{BASE_PATH}/data/hqs/in_download.txt\"\n",
    "if not os.path.exists(log_path):\n",
    "    with open(log_path, \"w+\") as file:\n",
    "        file.write(\"\")\n",
    "\n",
    "already_downloaded = []\n",
    "with open(log_path) as file:\n",
    "    already_downloaded = [f.replace(\"\\n\", \"\").split(\"\\t\")[0] for f in file.readlines()]\n",
    "\n",
    "for i in range(0, len(title_dataset)):\n",
    "    # Exibe log de execução\n",
    "    print(f\"Acessando {i+1}/{len(title_dataset)}\", end=\"\\r\")\n",
    "\n",
    "    # Recupera dados da publicação\n",
    "    url = title_dataset[\"page_link\"].values[i]\n",
    "    name = title_dataset[\"name\"].values[i].replace(\":\", \" \").replace(\"?\", \"\")\\\n",
    "        .replace(\"/\", \"-\").replace(\"*\", \"-\").replace(\"\\\"\", \"\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Verifica se ele já foi processado\n",
    "    if name in already_downloaded:\n",
    "        continue\n",
    "\n",
    "    # Abre o driver e faz o download do código fonte\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if first_visit:\n",
    "        first_visit = False\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "    with open(f\"{BASE_PATH}/data/hqs/download/{i} - {name}.html\", \"w+\") as file:\n",
    "        file.write(driver.page_source)\n",
    "\n",
    "    # Registra no log o download\n",
    "    total_time = time.time() - start_time\n",
    "    with open(log_path, \"a\") as file:\n",
    "        file.write(f\"{name}\\t{total_time}\\n\")\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3: Processamento\n",
    "\n",
    "Tudo o que foi executado anteriormente teve por objetivo chegar nesta etapa. Visto que estamos lidando com páginas web que foram baixadas localmente em um diretório específico, este bloco vai iterar sobre todos os arquivos existentes no diretório e executar a raspagem, padronização e compilação dos dados. Neste ponto as particularidades de cada página interferem nos comandos, sendo necessário tratar todos os casos (página não encontrada, produto indisponível e apenas formato digital) para que se tenha o scrip mais genérico e eficiente possível.\n",
    "\n",
    "Apesar de não ser tão lento quanto a Parte 2, este processo também requer um certo tempo de execução, que obviamente cresce com o aumento da quantidade de páginas baixadas, porém não em uma escala tão pronunciada. Ao final, o arquivo *prices_dataset.csv* é gerado com todos os dados perfeitamente processados e prontos para seguirem para os processos de análise estatística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando 12630/12630\r"
     ]
    }
   ],
   "source": [
    "pages = os.listdir(f\"{BASE_PATH}/data/hqs/download\")\n",
    "dataset = []\n",
    "skip = []\n",
    "\n",
    "month_to_number = {\n",
    "    \"janeiro\": \"01\", \"fevereiro\": \"02\", \"março\": \"03\", \"abril\": \"04\",\n",
    "    \"maio\": \"05\", \"junho\": \"06\", \"julho\": \"07\", \"agosto\": \"08\",\n",
    "    \"setembro\": \"09\", \"outubro\": \"10\", \"novembro\": \"11\", \"dezembro\": \"12\"\n",
    "}\n",
    "\n",
    "for page in pages:\n",
    "    # Inicia um registro e acessa a página baixada\n",
    "    register = dict()\n",
    "    page_path = f\"{BASE_PATH}/data/hqs/download/{page}\"\n",
    "\n",
    "    # Log de execução\n",
    "    print(f\"Processando {pages.index(page) + 1}/{len(pages)}\", end=\"\\r\")\n",
    "\n",
    "    with open(page_path) as file:\n",
    "        download_at = datetime.fromtimestamp(os.path.getctime(page_path)).isoformat()\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    # Identifica se o livro está fora de estoque. Neste caso, a página não contêm\n",
    "    # a informação de preço, que é fundamental para esta análise. Logo, este item\n",
    "    # não vai compor o dataset final.\n",
    "    if soup.find(\"div\", {\"id\": \"outOfStock\"}) is not None:\n",
    "        continue\n",
    "\n",
    "    # Algumas páginas vieram sem conteúdo, desta forma não existe motivo para\n",
    "    # processá-las. Esta flag remove estas páginas da lista de execução.\n",
    "    try:\n",
    "        if soup.find(\"title\").text.strip() in [\n",
    "            \"Não foi possível encontrar esta página\",\n",
    "            \"Amazon.com.br\",\n",
    "            \"Amazon Clinic\"\n",
    "        ]:\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # Verifica se é um livro digital, encontrando qual versão está selecionada na\n",
    "    # página. A seleção é uma div (cartão) com nome do formato e o menor preço.\n",
    "    selected_format = None\n",
    "    for element in soup.find_all(\"div\", {\"class\": \"a-ws-row\"}):\n",
    "        selected_format = element.find(\"div\", {\"class\":\"selected\"})\n",
    "        if selected_format is not None:\n",
    "            break\n",
    "\n",
    "    if selected_format is None:\n",
    "        continue\n",
    "\n",
    "    book_format = selected_format.find(\"span\", {\"class\": \"slot-title\"})\\\n",
    "        .find(\"span\").text.upper().strip()\n",
    "\n",
    "    if book_format == \"KINDLE\":\n",
    "        continue\n",
    "\n",
    "    # Registra o formato encontrado. Importante que apenas formatos físicos são\n",
    "    # incluídos neste conjunto de dados.\n",
    "    register[\"format\"] = book_format\n",
    "\n",
    "    # Recupera data de acesso, que é a data em que foi feito o download da página.\n",
    "    register[\"download_at\"] = download_at\n",
    "\n",
    "    # Recupera o preço de venda acessando o valor original (preço de capa). Em casos\n",
    "    # em não existe preço de capa, apenas uma listagem de marketplace, utiliza o\n",
    "    # preço exibido no cartão selecionado.\n",
    "    element = soup.find(\"span\", {\"id\": \"listPrice\"})\n",
    "    if element is None:\n",
    "        element = selected_format.find(\"span\", {\"class\": \"slot-price\"})\n",
    "\n",
    "    full_price = float(re.findall(\"[0-9]+,[0-9]+\", element.text.strip())[0].replace(\",\", \".\"))\n",
    "    register[\"full_price\"] = full_price\n",
    "\n",
    "    # Recupera o nome\n",
    "    name = soup.find(\"span\", {\"id\": \"productTitle\"}).text.strip()\n",
    "    register[\"name\"] = name\n",
    "\n",
    "    # Organiza a lista de informações genéricas\n",
    "    list_data = dict()\n",
    "    detail_div = soup.find(\"div\", {\"id\": \"detailBullets_feature_div\"})\n",
    "    if detail_div is None:\n",
    "        continue\n",
    "\n",
    "    elements = detail_div.find_all(\"span\", {\"class\": \"a-list-item\"})\n",
    "    for element in elements:\n",
    "        fields = element.text.replace(\"\\n\", \" \").replace(\"\\u200f\", \"\")\\\n",
    "            .replace(\"\\u200e\", \"\").replace(\"  \", \"\").strip().split(\":\")\n",
    "        if len(fields) < 2:\n",
    "            continue\n",
    "        list_data[fields[0].strip().upper()] = fields[1].strip()\n",
    "\n",
    "    # Recupera o nome editora, ignorando o registro caso não exista esta\n",
    "    # informação na página.\n",
    "    if \"EDITORA\" not in list_data.keys():\n",
    "        continue\n",
    "    publisher = list_data[\"EDITORA\"].split(\";\")[0].split(\"(\")[0]\n",
    "    register[\"publisher\"] = publisher.upper().strip()\n",
    "\n",
    "    # Recupera a data de lançamento da edição.\n",
    "    i = list_data[\"EDITORA\"].index(\"(\")\n",
    "    date_parts = list_data[\"EDITORA\"][i+1:-1].split(\" \")\n",
    "    release_date = f\"{date_parts[2]}-{month_to_number[date_parts[1]]}-{date_parts[0]}\"\n",
    "    register[\"release_date\"] = release_date\n",
    "\n",
    "    # Recupera o número de páginas da edição. Acessa uma div específica (cartão)\n",
    "    # que exibe esta informação. A expressão regular captura somente o número. Se\n",
    "    # não existir informação do número de páginas, ignora o registro, pois se trata\n",
    "    # de um anúncio falho.\n",
    "    element = soup.find(\"div\", {\"id\": \"rpi-attribute-book_details-fiona_pages\"})\n",
    "    if element is None:\n",
    "        skip.append(page)\n",
    "        continue\n",
    "\n",
    "    element.find(\"div\", {\"class\": \"a-section a-spacing-none a-text-center rpi-attribute-value\"})\\\n",
    "        .find(\"span\")\n",
    "    register[\"pages\"] = int(re.findall(\"[0-9]+\", element.text)[0])\n",
    "\n",
    "    # Recupera dimensões no formato altura x largura x profundidade. Caso não\n",
    "    # exista a informação, atribui null\n",
    "    dimension = None\n",
    "    if \"DIMENSÕES\" in list_data.keys():\n",
    "        dimension = list_data[\"DIMENSÕES\"]\n",
    "    register[\"dimensions\"] = dimension\n",
    "\n",
    "    # Recupera os registros de ISBN. Caso não existam, atribui null.\n",
    "    isbn_10, isbn_13 = None, None\n",
    "    if \"ISBN-10\" in list_data.keys():\n",
    "        isbn_10 = list_data[\"ISBN-10\"]\n",
    "\n",
    "    if \"ISBN-13\" in list_data.keys():\n",
    "        isbn_13 = list_data[\"ISBN-13\"]\n",
    "\n",
    "    register[\"ISBN-10\"] = isbn_10\n",
    "    register[\"ISBN-13\"] = isbn_13\n",
    "\n",
    "    # Recupera nota das avaliações dos clientes. Quando não existe avaliação, preenche\n",
    "    # com o valor null.\n",
    "    customer_review = None\n",
    "    element = soup.find(\"div\", {\"id\": \"averageCustomerReviews\"})\n",
    "    if element is not None:\n",
    "        element = element.find(\"span\", {\"class\": \"a-size-base\"})\n",
    "        customer_review = float(element.text.strip().replace(\",\", \".\"))\n",
    "    register[\"customers_review\"] = customer_review\n",
    "\n",
    "    # Recupera a descrição\n",
    "    review = []\n",
    "    for element in soup.find(\"div\", {\"id\": \"bookDescription_feature_div\"}).children:\n",
    "        review.append(element.text.replace(\"\\n\", \" \").strip())\n",
    "    register[\"about\"] = \" \".join(review).replace(\"  \", \" \").strip()\n",
    "\n",
    "    dataset.append(register)\n",
    "\n",
    "# Organiza em dataframe e salva resultado\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.to_csv(f\"{BASE_PATH}/data/hqs/prices_dataset.csv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pós Análise: Transformações e Finalização\n",
    "\n",
    "Com as necessidades que surgem das primeiras análises exploratórias, a criação de mais features a partir do processamento de texto se faz necessário. A principal transformação é a obtenção dos dados relativos às dimensões do produto.\n",
    "\n",
    "Também é responsável por finalizar o dataset para ser disponibilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(f\"{BASE_PATH}/data/hqs/prices_dataset.csv\", sep=\"\\t\")\n",
    "height, width, depth = [], [], []\n",
    "\n",
    "for i in range(0, len(dataset)):\n",
    "    dimensions = [None, None, None]\n",
    "    if type(dataset[\"dimensions\"].values[i]) == str:\n",
    "        parts = dataset[\"dimensions\"].values[i].split(\"x\")\n",
    "        dimensions = [\n",
    "            float(parts[0].strip()),\n",
    "            float(parts[1].strip()),\n",
    "            float(re.findall(\"[0-9|.]+\", parts[2])[0])\n",
    "        ]\n",
    "        dimensions.sort(reverse=True)\n",
    "    \n",
    "    height.append(dimensions[0])\n",
    "    width.append(dimensions[1])\n",
    "    depth.append(dimensions[2])\n",
    "\n",
    "dataset[\"height\"] = height\n",
    "dataset[\"width\"] = width\n",
    "dataset[\"depth\"] = depth\n",
    "\n",
    "dataset.to_csv(f\"{BASE_PATH}/data/hqs/prices_dataset.csv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(f\"{BASE_PATH}/data/hqs/prices_dataset.csv\", sep=\"\\t\")\n",
    "dataset = dataset.drop(\"dimensions\", axis=1)\n",
    "dataset.to_csv(f\"{BASE_PATH}/data/hqs/datasets/volumes.csv\", index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
