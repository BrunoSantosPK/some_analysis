{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_PATH = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
    "    #\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "    \"Accept-Encoding\":\"gzip, deflate\",\n",
    "    \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"DNT\":\"1\",\"Connection\":\"close\",\n",
    "    \"Upgrade-Insecure-Requests\":\"1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers = [\n",
    "    \"pipoca+e+nanquim\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://www.amazon.com.br/s?i=stripbooks&rh=p_30%3Apipoca+e+nanquim&s=relevanceexprank&Adv-Srch-Books-Submit.x=0&Adv-Srch-Books-Submit.y=0&unfiltered=1&ref=sr_adv_b\"\n",
    "#url = f\"https://www.amazon.com.br/s?ie=UTF8&rh=p_30%3A{publishers[0]}&s=date-desc-rank\"\n",
    "req = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "with open(f\"{BASE_PATH}/data/temp.html\", \"w+\") as file:\n",
    "    file.write(str(soup.prettify()))\n",
    "\n",
    "soup.find_all(\"div\", {\"class\": \"s-search-results\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "BASE_PATH = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers = [\n",
    "    \"pipoca+e+nanquim\",\n",
    "    \"newpop\",\n",
    "    \"jbc\",\n",
    "    \"panini\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dataset = []\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "for publisher in publishers:\n",
    "    # Acessa a página de busca utilizando query param\n",
    "    url = f\"https://www.amazon.com.br/s?ie=UTF8&rh=p_30%3A{publisher}&s=date-desc-rank\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    while True:\n",
    "        # Recupera lista de resultados\n",
    "        elements = driver.find_element(By.CLASS_NAME, \"s-result-list\")\n",
    "        books = elements.find_elements(By.XPATH, \".//div[@data-component-type='s-search-result']\")\n",
    "\n",
    "        # Percorre os livros encontrados e salva dados\n",
    "        for book in books:\n",
    "            row = {\"publisher\": \" \".join(publisher.split(\"+\")), \"extract_at\": date.today().isoformat()}\n",
    "\n",
    "            book_link = book.find_element(By.XPATH, \".//a[@class='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal']\")\n",
    "            row[\"page_link\"] = book_link.get_attribute(\"href\")\n",
    "\n",
    "            book_name = book.find_element(By.XPATH, \".//span[@class='a-size-base-plus a-color-base a-text-normal']\")\n",
    "            row[\"name\"] = book_name.get_attribute(\"innerHTML\")\n",
    "\n",
    "            title_dataset.append(row)\n",
    "\n",
    "        # Faz a troca de página, utilizando o botão de \"próxima\"\n",
    "        old_url = driver.current_url\n",
    "        driver.find_element(By.CLASS_NAME, \"s-pagination-next\").click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        if driver.current_url == old_url:\n",
    "            break\n",
    "\n",
    "title_dataset = pd.DataFrame(title_dataset)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os dados primários para utilização posterior\n",
    "title_dataset.to_csv(f\"{BASE_PATH}/data/hqs/titles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/data/hqs/download/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     22\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(soup\u001b[38;5;241m.\u001b[39mprettify()))\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "title_dataset = pd.read_csv(f\"{BASE_PATH}/data/hqs/titles.csv\")\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
    "    #\"Accept-Encoding\":\"gzip, deflate\",\n",
    "    \"Accept\":\"*/*\",\n",
    "    \"Cookie\": \"i18n-prefs=BRL; session-id=140-7276983-7988533; session-id-time=2082787201l\"\n",
    "    #\"DNT\":\"1\",\"Connection\":\"close\",\n",
    "    #\"Upgrade-Insecure-Requests\":\"1\"\n",
    "}\n",
    "\n",
    "for i in range(0, len(title_dataset)):\n",
    "    # Recupera dados da edição\n",
    "    url = title_dataset[\"page_link\"].values[i]\n",
    "    name = title_dataset[\"name\"].values[i].replace(\":\", \" \").replace(\"?\", \"\")\n",
    "\n",
    "    # Acessa a página\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "    # Salva o HTML para processamento futuro\n",
    "    with open(f\"{BASE_PATH}/data/hqs/download/{i} - {name}.html\", \"w+\") as file:\n",
    "        file.write(str(soup.prettify()))\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
